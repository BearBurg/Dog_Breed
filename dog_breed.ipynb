{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import matplotlib.pylab as plt\n",
    "import os\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_label = {}\n",
    "with open('dataset/labels.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in reader:\n",
    "        image_name = row[0]\n",
    "        label = row[1]\n",
    "        \n",
    "        if label == 'breed':\n",
    "            continue\n",
    "        \n",
    "        if label not in image_label:\n",
    "            image_label[label] = set()\n",
    "            \n",
    "        image_label[label].add(image_name)\n",
    "\n",
    "\n",
    "# sorted_dict = sorted(image_label.keys())\n",
    "# print(sorted_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dist = {}\n",
    "for k,v in image_label.items():\n",
    "    label_dist[k] = len(v)\n",
    "\n",
    "sorted_image_label = sorted(label_dist.items()) # sorted by key, return a list of tuples\n",
    "\n",
    "print(sorted_image_label)\n",
    "\n",
    "l, c = list(label_dist.keys()), list(label_dist.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,70))\n",
    "plt.barh(l,c)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test_label(path):\n",
    "    labels = []\n",
    "    for foldername in os.listdir(path):\n",
    "#         print(foldername)\n",
    "        if foldername != '.DS_Store':\n",
    "            names = foldername.split('-')\n",
    "            l = len(names)\n",
    "            label = names[1]\n",
    "            for i in range(2,l):\n",
    "                label += '-' + names[i]\n",
    "            labels.append(label)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_test_label('Images')\n",
    "labels.sort()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all/train\n",
    "def image_size(train_dir):\n",
    "    width = []\n",
    "    height = []\n",
    "    for image_dir in os.listdir(train_dir):\n",
    "        if image_dir != '.DS_Store':\n",
    "            image = PIL.Image.open(train_dir + '/' + image_dir)\n",
    "            width.append(image.size[0])\n",
    "            height.append(image.size[1])\n",
    "    return width,height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "3264\n"
     ]
    }
   ],
   "source": [
    "width,height = image_size('dataset/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222\n"
     ]
    }
   ],
   "source": [
    "print(len(width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "width.sort()\n",
    "l, c = list(range(len(width))), width\n",
    "plt.ylim(0,1500)\n",
    "# plt.figure(figsize=(30,70))\n",
    "plt.bar(l,c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "height.sort()\n",
    "l, c = list(range(len(height))), height\n",
    "plt.ylim(0,1500)\n",
    "# plt.figure(figsize=(30,70))\n",
    "plt.bar(l,c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image \n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.applications import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import ImageFile  \n",
    "from tqdm import tqdm\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True    \n",
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 870/870 [00:05<00:00, 165.70it/s]\n",
      "100%|██████████| 1739/1739 [00:08<00:00, 211.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_files, train_labels = load_dataset('dataset_small/train')\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "test_files, test_labels = load_dataset('dataset_small/test')\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xceptionData = np.load('DogXceptionData.npz')\n",
    "xceptionTrain = xceptionData['train']\n",
    "# xceptionValid = xceptionData['valid']\n",
    "xceptionTest = xceptionData['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50Data = np.load('DogResnet50Data.npz')\n",
    "resnet50Train = resnet50Data['train']\n",
    "resnet50Test = resnet50Data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50Data_1 = np.load('resnet50data.npz')\n",
    "resnet50Train_1 = resnet50Data['train']\n",
    "resnet50Test_1 = resnet50Data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_Xception(tensor):\n",
    "    return Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Xception_transfer_learning(train_Xception,train_targets,length):\n",
    "    Xception_model = Sequential()\n",
    "    Xception_model.add(GlobalAveragePooling2D(input_shape=train_Xception.shape[1:]))\n",
    "    Xception_model.add(Dense(512, activation='relu'))\n",
    "    Xception_model.add(Dropout(0.5))\n",
    "    Xception_model.add(Dense(length, activation='softmax'))\n",
    "\n",
    "    Xception_model.summary()\n",
    "    Xception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "    Xception_model.fit(train_Xception, train_targets,epochs=20, batch_size=40, callbacks=[checkpointer, early_stop], verbose=1)\n",
    "    return Xception_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, labels = load_dataset('dogImages/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_f, test_labels = load_dataset('dogImages/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_23  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 1,117,317\n",
      "Trainable params: 1,117,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 37s 6ms/step - loss: 1.1829 - acc: 0.7018\n",
      "Epoch 2/20\n",
      " 120/6680 [..............................] - ETA: 11s - loss: 0.3975 - acc: 0.8583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - 8s 1ms/step - loss: 0.4594 - acc: 0.8558\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 5s 822us/step - loss: 0.3172 - acc: 0.8930\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 6s 827us/step - loss: 0.2310 - acc: 0.9223\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 5s 810us/step - loss: 0.1802 - acc: 0.9394\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 5s 820us/step - loss: 0.1345 - acc: 0.9530\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 5s 814us/step - loss: 0.1164 - acc: 0.9632\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 6s 844us/step - loss: 0.0908 - acc: 0.9692\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 6s 837us/step - loss: 0.0748 - acc: 0.9759\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 6s 935us/step - loss: 0.0646 - acc: 0.9790\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 6s 972us/step - loss: 0.0678 - acc: 0.9793\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 7s 1ms/step - loss: 0.0493 - acc: 0.9814\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 6s 919us/step - loss: 0.0423 - acc: 0.9871\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 6s 914us/step - loss: 0.0385 - acc: 0.9882\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 6s 940us/step - loss: 0.0343 - acc: 0.9915\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 6s 953us/step - loss: 0.0405 - acc: 0.9897\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 6s 859us/step - loss: 0.0335 - acc: 0.9916 1s - loss:\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 6s 886us/step - loss: 0.0277 - acc: 0.9931\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 6s 946us/step - loss: 0.0333 - acc: 0.9922\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 7s 973us/step - loss: 0.0296 - acc: 0.9937\n"
     ]
    }
   ],
   "source": [
    "Xception_model = Xception_transfer_learning(xceptionTrain,labels,133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranfer_learning(trainingData,trainLabels,length):\n",
    "    model = Sequential()\n",
    "    model.add(GlobalAveragePooling2D(input_shape=trainingData.shape[1:]))\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.7))\n",
    "    model.add(Dense(length, activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#     checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "#                                verbose=1, save_best_only=True)\n",
    "\n",
    "#     early_stop = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=0, mode='auto')\n",
    "\n",
    "    model.fit(trainingData, trainLabels, epochs=20, batch_size=40, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_35  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 23s 3ms/step - loss: 1.8492 - acc: 0.5731\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 3s 408us/step - loss: 0.4852 - acc: 0.8555\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 3s 429us/step - loss: 0.2724 - acc: 0.9228\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 3s 416us/step - loss: 0.1694 - acc: 0.9507\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 3s 465us/step - loss: 0.1109 - acc: 0.9731 0s - loss: 0.1088 - acc: 0\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 3s 447us/step - loss: 0.0719 - acc: 0.9819\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 3s 439us/step - loss: 0.0512 - acc: 0.9888\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 3s 387us/step - loss: 0.0360 - acc: 0.9928\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 3s 380us/step - loss: 0.0264 - acc: 0.9946\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 3s 383us/step - loss: 0.0208 - acc: 0.9961\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 3s 386us/step - loss: 0.0153 - acc: 0.9978\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 3s 429us/step - loss: 0.0119 - acc: 0.9973\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 3s 473us/step - loss: 0.0091 - acc: 0.9981\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 2s 374us/step - loss: 0.0082 - acc: 0.9988\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 3s 398us/step - loss: 0.0066 - acc: 0.9987\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 3s 436us/step - loss: 0.0057 - acc: 0.9982\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 3s 404us/step - loss: 0.0043 - acc: 0.9990\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 3s 461us/step - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 3s 382us/step - loss: 0.0048 - acc: 0.9985\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 3s 378us/step - loss: 0.0049 - acc: 0.9988\n"
     ]
    }
   ],
   "source": [
    "restnet_model = tranfer_learning(resnet50Train,labels,133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.2535885167464\n"
     ]
    }
   ],
   "source": [
    "res50 = [np.argmax(restnet_model.predict(np.expand_dims(test,axis = 0))) for test in resnet50Test]\n",
    "accuracy_resnet = 100* np.sum(res50 == np.argmax(test_labels, axis = 1))/len(test_labels)\n",
    "print(accuracy_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_36  (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 21s 3ms/step - loss: 1.8530 - acc: 0.5540\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 3s 375us/step - loss: 0.4741 - acc: 0.8635\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 3s 383us/step - loss: 0.2718 - acc: 0.9213\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 3s 382us/step - loss: 0.1633 - acc: 0.9533\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 3s 384us/step - loss: 0.1082 - acc: 0.9708\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 3s 379us/step - loss: 0.0747 - acc: 0.9829\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 3s 382us/step - loss: 0.0509 - acc: 0.9882\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 3s 423us/step - loss: 0.0358 - acc: 0.9937\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 3s 390us/step - loss: 0.0272 - acc: 0.9945\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 3s 478us/step - loss: 0.0190 - acc: 0.9972\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 3s 435us/step - loss: 0.0154 - acc: 0.9976\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 3s 506us/step - loss: 0.0114 - acc: 0.9982\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 4s 609us/step - loss: 0.0094 - acc: 0.9984\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 3s 410us/step - loss: 0.0083 - acc: 0.9981\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 3s 422us/step - loss: 0.0073 - acc: 0.9987\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 3s 430us/step - loss: 0.0055 - acc: 0.9985\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 3s 403us/step - loss: 0.0051 - acc: 0.9987\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 3s 444us/step - loss: 0.0051 - acc: 0.9985\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 3s 381us/step - loss: 0.0043 - acc: 0.9987\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 3s 385us/step - loss: 0.0040 - acc: 0.9990\n"
     ]
    }
   ],
   "source": [
    "resnet_model1 = tranfer_learning(resnet50Train_1,labels,133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.41626794258373\n"
     ]
    }
   ],
   "source": [
    "res1 = [np.argmax(resnet_model1.predict(np.expand_dims(test,axis = 0))) for test in resnet50Test_1]\n",
    "accuracy_resnet = 100* np.sum(res1 == np.argmax(test_labels, axis = 1))/len(test_labels)\n",
    "print(accuracy_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model,testData,testLabel,m):\n",
    "    i = 0\n",
    "    a = 0\n",
    "    for test in testData:\n",
    "        arr = model.predict(np.expand_dims(test,axis = 0))\n",
    "        idx = np.argsort(-arr[0])[:m]\n",
    "        if np.argmax(testLabel[i]) in idx:\n",
    "            a += 1\n",
    "        i += 1\n",
    "    return a/len(testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 3 accuracy is: 0.9629186602870813\n"
     ]
    }
   ],
   "source": [
    "m = 3\n",
    "acc = calculate_accuracy(resnet_model1,resnet50Test_1,test_labels,m)\n",
    "print('top ' + str(m)+ ' accuracy is: ' + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(label_dir,model,test,m):\n",
    "    labelList = []\n",
    "    labels = [item[16:-1] for item in sorted(glob(label_dir))]\n",
    "#     print(labels)\n",
    "    arr = model.predict(np.expand_dims(test,axis = 0))\n",
    "    idx = np.argsort(-arr[0])[:m]\n",
    "    for index in idx:\n",
    "        labelList.append(labels[index])\n",
    "    return labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2048)\n",
      "['101.Maltese', '024.Bichon_frise', '008.American_staffordshire_terrier', '091.Japanese_chin', '016.Beagle']\n"
     ]
    }
   ],
   "source": [
    "label_dir = \"dogImages/train/*/\"\n",
    "# test_dir = 'Images/n02086910-papillon/n02086910_695.jpg'\n",
    "test_dir = 'dogImages/test/015.Basset_hound/Basset_hound_01071.jpg'\n",
    "model = resnet_model1\n",
    "test = preprocess_input(path_to_tensor(test_dir))\n",
    "# print(test.shape)\n",
    "test = ResNet50(weights='imagenet', include_top=False,pooling = 'avg').predict(test)\n",
    "# test = resnet50Test_1[0]\n",
    "test = np.expand_dims(test,axis = 0)\n",
    "print(test.shape)\n",
    "m = 5\n",
    "labelList = make_prediction(label_dir,model,test,m)\n",
    "print(labelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_Xception(train_dir, batch_size,class_num,epoch = 10, samples_per_epoch = 100, transformation_ratio = .05):\n",
    "    xception_width = 299\n",
    "    xception_height = 299\n",
    "#     class_num = 10\n",
    "    base_model = Xception(input_shape=(xception_width, xception_height, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "    # Top Model Block\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    predictions = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "    # add your top layer block to base model\n",
    "    model = Model(base_model.input, predictions)\n",
    "#     print(model.summary())\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False \n",
    "#     train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "#                                        rotation_range=transformation_ratio,\n",
    "#                                        shear_range=transformation_ratio,\n",
    "#                                        zoom_range=transformation_ratio,\n",
    "#                                        cval=transformation_ratio,\n",
    "#                                        horizontal_flip=True,\n",
    "#                                        vertical_flip=True)\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#     validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                        target_size=(xception_width, xception_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')\n",
    "    filenames = train_generator.filenames\n",
    "    nb_sample = len(filenames)\n",
    "    print(nb_sample)\n",
    "    model.compile(optimizer='nadam',\n",
    "                  loss='categorical_crossentropy',  # categorical_crossentropy if multi-class classifier\n",
    "                  metrics=['accuracy'])\n",
    "    checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=0, mode='auto')\n",
    "    model.fit_generator(train_generator,\n",
    "                        samples_per_epoch=100,\n",
    "                        nb_epoch = epoch,callbacks=[checkpointer, early_stop])\n",
    "    print('done training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 870 images belonging to 10 classes.\n",
      "870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=5, epochs=5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 [==============================] - 65s 13s/step - loss: 2.0961 - acc: 0.3000\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:434: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 47s 9s/step - loss: 1.3748 - acc: 0.8100\n",
      "Epoch 3/5\n",
      "5/5 [==============================] - 40s 8s/step - loss: 0.9841 - acc: 0.8900\n",
      "Epoch 4/5\n",
      "5/5 [==============================] - 39s 8s/step - loss: 0.5879 - acc: 0.9400\n",
      "Epoch 5/5\n",
      "5/5 [==============================] - 45s 9s/step - loss: 0.3475 - acc: 0.9900\n",
      "done training\n"
     ]
    }
   ],
   "source": [
    "model = transfer_learning_Xception('dataset_small/train',20,10,epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_xception(test_dir):\n",
    "    xception_width = 299\n",
    "    xception_height = 299\n",
    "    batch_size = 20\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    print(test_datagen)\n",
    "    test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                        target_size=(xception_width, xception_height),\n",
    "                                                        batch_size=1739,\n",
    "                                                        class_mode='categorical')\n",
    "    X,y =  test_generator.next()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.preprocessing.image.ImageDataGenerator object at 0x1af99dc0b8>\n",
      "Found 1739 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data,test_label = test_xception('dataset_small/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3450258769407706\n"
     ]
    }
   ],
   "source": [
    "res = [np.argmax(model.predict(np.expand_dims(test,axis = 0))) for test in test_data]\n",
    "a = 100* np.sum(res == np.argmax(test_label, axis = 1))/len(test_label)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [1,2,3,4,5,6]\n",
    "arr = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "idx = (-arr).argsort()[:1]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
